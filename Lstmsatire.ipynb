{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from vecstack import stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>source</th>\n",
       "      <th>original_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>lang</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user_mentions</th>\n",
       "      <th>place</th>\n",
       "      <th>place_coord_boundaries</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>1196513648929378305</td>\n",
       "      <td>Mon Nov 18 19:40:58 +0000 2019</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>RT @djordjepadejski: Serbian reporter @miodrag...</td>\n",
       "      <td>Serbian reporter accused causing cardiovascula...</td>\n",
       "      <td>Sentiment(polarity=0.0, subjectivity=0.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>TamaraSinobad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>djordjepadejski, miodrag_sovilj, avucic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>1196478851175649281</td>\n",
       "      <td>Mon Nov 18 17:22:42 +0000 2019</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>RT @djordjepadejski: Serbian reporter @miodrag...</td>\n",
       "      <td>Serbian reporter accused causing cardiovascula...</td>\n",
       "      <td>Sentiment(polarity=0.0, subjectivity=0.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>1od5milionaV1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>djordjepadejski, miodrag_sovilj, avucic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>1197467032637313024</td>\n",
       "      <td>Thu Nov 21 10:49:23 +0000 2019</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/#!/download/ipad\" ...</td>\n",
       "      <td>These need to start coming with a #NotTheOnion...</td>\n",
       "      <td>These need start coming watermark hashtag</td>\n",
       "      <td>Sentiment(polarity=0.0, subjectivity=0.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Drizztdj</td>\n",
       "      <td>False</td>\n",
       "      <td>NotTheOnion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Minneapolis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>1197345596400656389</td>\n",
       "      <td>Thu Nov 21 02:46:50 +0000 2019</td>\n",
       "      <td>&lt;a href=\"Http://www.youtbe.com\" rel=\"nofollow\"...</td>\n",
       "      <td>RT @BryanDawsonUSA: Meanwhile, Fox News watche...</td>\n",
       "      <td>Meanwhile Fox News watchers debating movie Fro...</td>\n",
       "      <td>Sentiment(polarity=0.2, subjectivity=0.2)</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>RealBryanDawson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BryanDawsonUSA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>1196474459428810752</td>\n",
       "      <td>Mon Nov 18 17:05:15 +0000 2019</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>Serbian reporter @miodrag_sovilj accused of ca...</td>\n",
       "      <td>Serbian reporter accused causing cardiovascula...</td>\n",
       "      <td>Sentiment(polarity=0.0, subjectivity=0.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>173</td>\n",
       "      <td>49</td>\n",
       "      <td>djordjepadejski</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>miodrag_sovilj, avucic</td>\n",
       "      <td>Stanford, CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                      created_at  \\\n",
       "262  1196513648929378305  Mon Nov 18 19:40:58 +0000 2019   \n",
       "278  1196478851175649281  Mon Nov 18 17:22:42 +0000 2019   \n",
       "120  1197467032637313024  Thu Nov 21 10:49:23 +0000 2019   \n",
       "142  1197345596400656389  Thu Nov 21 02:46:50 +0000 2019   \n",
       "285  1196474459428810752  Mon Nov 18 17:05:15 +0000 2019   \n",
       "\n",
       "                                                source  \\\n",
       "262  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "278  <a href=\"http://twitter.com/download/android\" ...   \n",
       "120  <a href=\"http://twitter.com/#!/download/ipad\" ...   \n",
       "142  <a href=\"Http://www.youtbe.com\" rel=\"nofollow\"...   \n",
       "285  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "\n",
       "                                         original_text  \\\n",
       "262  RT @djordjepadejski: Serbian reporter @miodrag...   \n",
       "278  RT @djordjepadejski: Serbian reporter @miodrag...   \n",
       "120  These need to start coming with a #NotTheOnion...   \n",
       "142  RT @BryanDawsonUSA: Meanwhile, Fox News watche...   \n",
       "285  Serbian reporter @miodrag_sovilj accused of ca...   \n",
       "\n",
       "                                            clean_text  \\\n",
       "262  Serbian reporter accused causing cardiovascula...   \n",
       "278  Serbian reporter accused causing cardiovascula...   \n",
       "120          These need start coming watermark hashtag   \n",
       "142  Meanwhile Fox News watchers debating movie Fro...   \n",
       "285  Serbian reporter accused causing cardiovascula...   \n",
       "\n",
       "                                     sentiment  polarity  subjectivity lang  \\\n",
       "262  Sentiment(polarity=0.0, subjectivity=0.0)       0.0           0.0   en   \n",
       "278  Sentiment(polarity=0.0, subjectivity=0.0)       0.0           0.0   en   \n",
       "120  Sentiment(polarity=0.0, subjectivity=0.0)       0.0           0.0   en   \n",
       "142  Sentiment(polarity=0.2, subjectivity=0.2)       0.2           0.2   en   \n",
       "285  Sentiment(polarity=0.0, subjectivity=0.0)       0.0           0.0   en   \n",
       "\n",
       "     favorite_count  retweet_count  original_author possibly_sensitive  \\\n",
       "262               0             49    TamaraSinobad                NaN   \n",
       "278               0             49    1od5milionaV1                NaN   \n",
       "120               0              0         Drizztdj              False   \n",
       "142               0              9  RealBryanDawson                NaN   \n",
       "285             173             49  djordjepadejski              False   \n",
       "\n",
       "        hashtags                            user_mentions         place  \\\n",
       "262          NaN  djordjepadejski, miodrag_sovilj, avucic           NaN   \n",
       "278          NaN  djordjepadejski, miodrag_sovilj, avucic           NaN   \n",
       "120  NotTheOnion                                      NaN   Minneapolis   \n",
       "142          NaN                           BryanDawsonUSA           NaN   \n",
       "285          NaN                   miodrag_sovilj, avucic  Stanford, CA   \n",
       "\n",
       "    place_coord_boundaries  Label  \n",
       "262                    NaN      0  \n",
       "278                    NaN      0  \n",
       "120                    NaN      0  \n",
       "142                    NaN      0  \n",
       "285                    NaN      0  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading the data from theonion vs nottheonion\n",
    "df = pd.read_csv('C:/Users/ASUS/Desktop/data/scrapedtweets/Tweets.csv')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting test-train-x-y-Features\n",
    "y = df[['Label']]\n",
    "X = df.iloc[:,[1, 4, 6, 7, 9, 10, 11, 12, 13, 14, 15]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will do an lstm on only the text of the data this corresponds to Xdl\n",
    "Xdl = df.iloc[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same thing splits...\n",
    "Xdl_train, Xdl_test, ydl_train, ydl_test = train_test_split(Xdl, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import for the lstm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting text to sequences could be later optimized with word2vec or something\n",
    "max_words = 1000\n",
    "max_len = 150\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(Xdl_train)\n",
    "sequences = tok.texts_to_sequences(Xdl_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining our network \n",
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 150, 50)           50000     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 96,337\n",
      "Trainable params: 96,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#RMSprop is op asf\n",
    "modeldl = RNN()\n",
    "modeldl.summary()\n",
    "modeldl.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 224 samples, validate on 56 samples\n",
      "Epoch 1/10\n",
      "224/224 [==============================] - 2s 8ms/step - loss: 0.6865 - acc: 0.5804 - val_loss: 0.6517 - val_acc: 0.8036\n",
      "Epoch 2/10\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 0.6222 - acc: 0.8214 - val_loss: 0.4868 - val_acc: 0.8036\n",
      "Epoch 3/10\n",
      "224/224 [==============================] - 0s 2ms/step - loss: 0.5473 - acc: 0.8214 - val_loss: 0.5102 - val_acc: 0.8036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x242cefe78d0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#meh\n",
    "modeldl.fit(sequences_matrix,ydl_train,batch_size=128,epochs=10,\n",
    "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 0s 1ms/step\n",
      "First model\n",
      " Test shit\n",
      "  Loss: 0.468\n",
      "  Accuracy: 0.886\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "test_sequences = tok.texts_to_sequences(Xdl_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "accr = model.evaluate(test_sequences_matrix,ydl_test)\n",
    "print('First model\\n Test shit\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-83346e075e50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m150\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtok\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtok\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'created_at'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0msequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtok\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'created_at'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'created_at'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    221\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                                             self.split)\n\u001b[0m\u001b[0;32m    224\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[1;34m(text, filters, lower, split)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \"\"\"\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "max_words = 1000\n",
    "max_len = 150\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train['created_at'])\n",
    "sequences = tok.texts_to_sequences(X_train['created_at'])\n",
    "X_train['created_at'] = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    KNeighborsClassifier(n_neighbors=5,\n",
    "                        n_jobs=-1),\n",
    "        \n",
    "    RandomForestClassifier(random_state=0, n_jobs=-1, \n",
    "                           n_estimators=100, max_depth=3),\n",
    "        \n",
    "    XGBClassifier(random_state=0, n_jobs=-1, learning_rate=0.1, \n",
    "                  n_estimators=100, max_depth=3),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264    Mon Nov 18 18:55:49 +0000 2019\n",
       "239    Tue Nov 19 00:19:18 +0000 2019\n",
       "304    Mon Nov 25 18:03:28 +0000 2019\n",
       "21     Tue Nov 26 22:28:41 +0000 2019\n",
       "135    Thu Nov 21 03:38:58 +0000 2019\n",
       "309    Mon Nov 25 10:45:50 +0000 2019\n",
       "49     Mon Nov 25 16:52:01 +0000 2019\n",
       "282    Mon Nov 18 17:13:05 +0000 2019\n",
       "289    Mon Nov 18 14:52:03 +0000 2019\n",
       "171    Wed Nov 20 18:28:07 +0000 2019\n",
       "223    Tue Nov 19 06:33:07 +0000 2019\n",
       "205    Tue Nov 19 15:46:08 +0000 2019\n",
       "286    Mon Nov 18 15:28:25 +0000 2019\n",
       "22     Tue Nov 26 22:19:11 +0000 2019\n",
       "261    Mon Nov 18 20:04:08 +0000 2019\n",
       "238    Tue Nov 19 00:21:30 +0000 2019\n",
       "140    Thu Nov 21 03:08:42 +0000 2019\n",
       "226    Tue Nov 19 05:33:14 +0000 2019\n",
       "66     Sun Nov 24 21:24:33 +0000 2019\n",
       "126    Thu Nov 21 05:11:33 +0000 2019\n",
       "12     Wed Nov 27 03:50:12 +0000 2019\n",
       "221    Tue Nov 19 06:34:05 +0000 2019\n",
       "316    Fri Nov 22 21:15:44 +0000 2019\n",
       "283    Mon Nov 18 17:11:03 +0000 2019\n",
       "252    Mon Nov 18 21:30:58 +0000 2019\n",
       "17     Tue Nov 26 22:59:20 +0000 2019\n",
       "167    Wed Nov 20 19:12:48 +0000 2019\n",
       "122    Thu Nov 21 06:30:11 +0000 2019\n",
       "276    Mon Nov 18 17:27:32 +0000 2019\n",
       "217    Tue Nov 19 09:31:36 +0000 2019\n",
       "                    ...              \n",
       "177    Wed Nov 20 15:36:16 +0000 2019\n",
       "99     Fri Nov 22 16:58:53 +0000 2019\n",
       "197    Tue Nov 19 20:32:52 +0000 2019\n",
       "243    Mon Nov 18 23:38:17 +0000 2019\n",
       "115    Thu Nov 21 13:26:28 +0000 2019\n",
       "265    Mon Nov 18 18:41:55 +0000 2019\n",
       "72     Sun Nov 24 17:11:58 +0000 2019\n",
       "25     Tue Nov 26 20:30:17 +0000 2019\n",
       "165    Wed Nov 20 19:40:41 +0000 2019\n",
       "335    Wed Nov 20 08:54:20 +0000 2019\n",
       "174    Wed Nov 20 17:15:03 +0000 2019\n",
       "337    Wed Nov 20 00:49:17 +0000 2019\n",
       "39     Tue Nov 26 13:31:04 +0000 2019\n",
       "193    Tue Nov 19 23:08:02 +0000 2019\n",
       "314    Sat Nov 23 05:21:03 +0000 2019\n",
       "88     Sat Nov 23 13:26:22 +0000 2019\n",
       "70     Sun Nov 24 19:12:12 +0000 2019\n",
       "87     Sat Nov 23 16:02:19 +0000 2019\n",
       "292    Wed Nov 27 01:12:22 +0000 2019\n",
       "242    Mon Nov 18 23:39:28 +0000 2019\n",
       "277    Mon Nov 18 17:26:01 +0000 2019\n",
       "211    Tue Nov 19 13:10:48 +0000 2019\n",
       "9      Wed Nov 27 04:12:33 +0000 2019\n",
       "195    Tue Nov 19 21:58:51 +0000 2019\n",
       "251    Mon Nov 18 21:42:18 +0000 2019\n",
       "323    Fri Nov 22 03:18:39 +0000 2019\n",
       "192    Tue Nov 19 23:40:31 +0000 2019\n",
       "117    Thu Nov 21 12:05:26 +0000 2019\n",
       "47     Mon Nov 25 20:54:26 +0000 2019\n",
       "172    Wed Nov 20 18:09:16 +0000 2019\n",
       "Name: created_at, Length: 280, dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
